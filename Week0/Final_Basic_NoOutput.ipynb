{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the GPU if it exists\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "SequentialNet = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724be2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, loader, optimizer, loss_fn, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining SequentialNet\")\n",
    "sequential_model = SequentialNet.to(device)\n",
    "optimizer_seq = optim.Adam(sequential_model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train(sequential_model, train_loader, optimizer_seq, loss_fn)\n",
    "test(sequential_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3506af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "class ManualNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ManualNet, self).__init__()\n",
    "        # TODO: Define your paramters using nn.Parameters (the layers)\n",
    "        self.w1 = nn.Parameter(torch.randn(128, 784) * 0.01) #First to second layer\n",
    "        self.w2 = nn.Parameter(torch.randn(64, 128) * 0.01) #Second to third layer\n",
    "        self.w3 = nn.Parameter(torch.randn(10, 64) * 0.01) #For final layer of size 10\n",
    "        self.b1 = nn.Parameter(torch.zeros(128))\n",
    "        self.b2 = nn.Parameter(torch.zeros(64))\n",
    "        self.b3 = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        # TODO: Do the forward pass using matrix multiplications and applying activation functions\n",
    "        out1 = x@self.w1.T + self.b1\n",
    "        out1 = (out1>0)*out1\n",
    "        out2 = out1@self.w2.T + self.b2\n",
    "        out2 = (out2>0)*out2\n",
    "        out3 = out2@self.w3.T + self.b3\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining ManualNet\")\n",
    "# TODO: Create a ManualNet object and call it manual_model. Train and test it\n",
    "manual_model = ManualNet().to(device)\n",
    "optimizer_manual = optim.Adam(manual_model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train(manual_model, train_loader, optimizer_manual, loss_fn)\n",
    "test(manual_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, loader, n=5):\n",
    "    model.eval()\n",
    "    x, y = next(iter(loader))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    preds = model(x).argmax(dim=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "        plt.title(f\"T:{y[i].item()} P:{preds[i].item()}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# visualize(manual_model, test_loader) # Uncomment this later\n",
    "visualize(sequential_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653e83d",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "* Load and preprocess CIFAR100 dataset (not CIFAR10)\n",
    "* Build a feedforward network for it. You can experiment around with number of layers and and neurons in each layer and different activation functions\n",
    "* You are allowed to use nn.functional. (convolutions _might_ make your accuracy better)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
