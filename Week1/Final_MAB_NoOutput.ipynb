{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6431241",
   "metadata": {},
   "source": [
    "The Multi-Armed Bandit (MAB) is a problem where an agent chooses among several options (arms), each with an unknown reward distribution, with the goal of maximizing cumulative reward over time. There are several ways to solve this problem, including Epsilon Greedy and Upper Confidence Bound (UCB). Letâ€™s go through both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the distributions\n",
    "class Arms:\n",
    "    def __init__(self, type, param1, param2):\n",
    "        self.type = type\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "\n",
    "    def get_reward(self):\n",
    "        if self.type == 'uniform':\n",
    "            return np.random.uniform(self.param1, self.param2)\n",
    "        if self.type == 'gaussian':\n",
    "            return np.random.normal(self.param1, self.param2)\n",
    "        if self.type == 'exponential':   \n",
    "            return np.random.exponential(1.0 / self.param1)\n",
    "        if self.type == 'beta':\n",
    "            return np.random.beta(self.param1, self.param2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365aa660",
   "metadata": {},
   "outputs": [],
   "source": [
    "arm1 = Arms('uniform', 5, 10)\n",
    "arm2 = Arms('gaussian', 6, 2)\n",
    "arm3 = Arms('exponential', 1/4, 0)\n",
    "arm4 = Arms('beta', 2, 5)\n",
    "arm5 = Arms('uniform', -5, 0)\n",
    "arm6 = Arms('gaussian', 10, 6)\n",
    "arm7 = Arms('exponential', 1/7, 0)\n",
    "arm8 = Arms('beta', 5, 2) \n",
    "\n",
    "arms = [arm1, arm2, arm3, arm4, arm5, arm6, arm7, arm8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251cd9a",
   "metadata": {},
   "source": [
    "In epsilon greedy, we define a number epsilon which dictates how often we explore and exploit.  \n",
    "Eploration consists of choosing a random arm and getting more information whilst exploiting is choosing the arm that you know gives the maximum rewards (based on the information you have so far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, arms, steps, epsilon):\n",
    "        self.arms = arms\n",
    "        self.epsilon = epsilon\n",
    "        self.steps = steps\n",
    "        # TODO: Initialize parameters\n",
    "        self.estimated_rewards = [0.0] * len(arms) # Estimated rewards of all arms\n",
    "        self.action_count = [0] * len(arms) # Number of times arm 'i' is chosen (for all arms)\n",
    "        self.total_reward = 0\n",
    "        self.rewards = []\n",
    "\n",
    "    def select_action(self):\n",
    "        # TODO: Implement epsilon greedy approach of choosing arms here\n",
    "        # Return action chosen (which arm to pick)\n",
    "        if np.random.random() < self.epsilon :\n",
    "            action = np.random.randint(0,len(self.arms))\n",
    "        else :\n",
    "            action = np.argmax(self.estimated_rewards)\n",
    "        return action\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        # TODO: Update estimated_rewards, action_count, total_reward, and rewards array here\n",
    "        self.action_count[action] += 1\n",
    "        n = self.action_count[action]\n",
    "        old_estimate = self.estimated_rewards[action]\n",
    "        new_estimate = old_estimate + (1 / n) * (reward - old_estimate)\n",
    "        self.estimated_rewards[action] = new_estimate\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.steps):\n",
    "            action = self.select_action()\n",
    "            reward = self.arms[action].get_reward()\n",
    "            self.update(action, reward)\n",
    "\n",
    "        return self.total_reward, self.estimated_rewards, self.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "epsilon = 0.1 # Play around with this value\n",
    "\n",
    "banditEG = EpsilonGreedy(arms, steps, epsilon)\n",
    "total_reward, estimated_reward, rewards = banditEG.run()\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Estimated means: {estimated_reward}\")\n",
    "\n",
    "# TODO: Plot average rewards over time (use np.cumsum)\n",
    "\n",
    "average_rewards = np.cumsum(rewards) / (np.arange(steps) + 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_rewards)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(f\"Epsilon = {epsilon}\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73563e8a",
   "metadata": {},
   "source": [
    "In upper confidence bound, we have the upper confidence estimate where we use both the estimated mean and the number of times we have picked that option. The more times you have chosen something, the more certain you are of its estimated mean. Using this we have 2 terms, the estimated mean and the confidence score. The constant c dictates how much the confidence score affects our choice.\n",
    "\n",
    "$UCB_t(a) = \\hat{Q}_t(a) + c \\cdot \\sqrt{\\frac{\\ln t}{N_t(a)}}$  \n",
    "$\\hat{Q}_t(a)$ is the estimated reward. $N_t(a)$ is the number of times arm $a$ has been picked and $t$ is the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb25161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound:\n",
    "    def __init__(self, arms, steps, c):\n",
    "        self.arms = arms\n",
    "        self.steps = steps\n",
    "        self.c = c\n",
    "        # TODO: Initialize parameters\n",
    "        self.estimated_rewards =  np.zeros(len(arms))# Estimated rewards of all arms\n",
    "        self.action_count = np.zeros(len(arms)) # Number of times arm 'i' is chosen (for all arms)\n",
    "        self.total_reward = 0\n",
    "        self.rewards = []\n",
    "        self.t = 0\n",
    "\n",
    "    def select_action(self, t):\n",
    "        if t < len(self.arms):\n",
    "            return t  \n",
    "\n",
    "        ucb_values = np.zeros(len(self.arms))\n",
    "        for i in range(len(self.arms)):\n",
    "            bonus = self.c * np.sqrt(np.log(t) / (self.action_count[i] + 1e-5))\n",
    "            ucb_values[i] = self.estimated_rewards[i] + bonus\n",
    "\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        self.action_count[action] += 1\n",
    "        n = self.action_count[action]\n",
    "        old_estimate = self.estimated_rewards[action]\n",
    "        self.estimated_rewards[action] += (reward - old_estimate) / n\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(self.total_reward / sum(self.action_count)) \n",
    "\n",
    "    def run(self):\n",
    "        for t in range(self.steps):\n",
    "            action = self.select_action(t)\n",
    "            reward = self.arms[action].get_reward()\n",
    "            self.update(action, reward)\n",
    "\n",
    "        return self.total_reward, self.estimated_rewards, self.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55af83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "c = 2 # Play around with this value\n",
    "\n",
    "banditUCB = UpperConfidenceBound(arms, steps, c)\n",
    "total_reward, estimated_reward, rewards = banditUCB.run()\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Estimated means: {estimated_reward}\")\n",
    "\n",
    "# TODO: Plot average rewards over time (use np.cumsum)\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(moving_avg)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Moving Average Reward\")\n",
    "plt.title(f\"UCB Moving Avg (window={window})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ad818",
   "metadata": {},
   "source": [
    "## Optional challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd198c32",
   "metadata": {},
   "source": [
    "1. if you already know the distribution the arms draw from(eg: gaussian), can you incorporate that knowledge to learn faster\n",
    "\n",
    "We will host a competition where we will run your agents and host a leaderboard(details in the next meet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2391fb0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
