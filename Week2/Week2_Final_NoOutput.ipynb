{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca209422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import pygame\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model which takes in the state as an input and outputs predicted q values for every possible action\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super().__init__()\n",
    "        # Add your architecture parameters here\n",
    "        # You can use nn.Functional\n",
    "        # Remember that the input is of size batch_size x state_space\n",
    "        # and the output is of size batch_size x action_space (ulta ho sakta hai dekh lo)\n",
    "        # TODO: Add code here\n",
    "        self.fc1 = nn.Linear(state_space, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, action_space)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: Complete based on your implementation\n",
    "        x = self.fc1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training neural networks, we split the data into batches.\n",
    "# To improve the training, we need to remove the \"correlation\" between game states\n",
    "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
    "# states at random which reduces the correlation.\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training logic for CartPole environment here\n",
    "# Remember to use the ExperienceBuffer and a target network\n",
    "# Details can be found in the book sent in the group\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "policy_net = DQN(state_space, action_space)\n",
    "target_net = DQN(state_space, action_space)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr = 1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "buffer = ExperienceBuffer(capacity=10000)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "target_update_freq = 10\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05  # was too high before\n",
    "epsilon_decay = 300  # faster decay helps earlier learning\n",
    "num_episodes = 500\n",
    "\n",
    "def get_epsilon(episode):\n",
    "    return epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    epsilon = get_epsilon(ep)\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = policy_net(state_tensor)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(1, keepdim=True)[0]\n",
    "                target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = criterion(q_values, target_q_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "    if ep % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    print(f\"Episode {ep}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cartpole_model(model, episodes=10, render=True):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run evaluation for cartpole here\n",
    "evaluate_cartpole_model(policy_net, episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
    "\n",
    "    def __init__(self, size=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.cell_size = 30\n",
    "        self.screen_size = self.size * self.cell_size\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
    "        self.observation_space = gym.spaces.Box(0, 1, shape=(3, self.size, self.size), dtype=np.float32)\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "\n",
    "        self.snake = deque()\n",
    "        self.food = None\n",
    "        self.direction = [1, 0]\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.snake.clear()\n",
    "        mid = self.size // 2\n",
    "        self.snake.appendleft([mid, mid])\n",
    "        self.direction = [1, 0]\n",
    "        self._place_food()\n",
    "        self.steps_since_last_food = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_init()\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        info = {}\n",
    "        self.steps_since_last_food += 1\n",
    "        old_direction = self.direction.copy()\n",
    "\n",
    "        # --- Direction update ---\n",
    "        intended = {\n",
    "            0: [1, 0],\n",
    "            1: [0, -1],\n",
    "            2: [-1, 0],\n",
    "            3: [0, 1],\n",
    "        }[action]\n",
    "\n",
    "        if intended == [-d for d in old_direction]:\n",
    "            reward -= 0.5  # stronger penalty for 180° turns\n",
    "        else:\n",
    "            self.direction = intended\n",
    "\n",
    "        head = self.snake[0]\n",
    "        new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
    "\n",
    "        done = False\n",
    "\n",
    "        # --- Wall collision ---\n",
    "        if not (0 <= new_head[0] < self.size and 0 <= new_head[1] < self.size):\n",
    "            reward -= 10\n",
    "            done = True\n",
    "        else:\n",
    "            # Use set for O(1) lookup\n",
    "            body_set = set(map(tuple, self.snake))\n",
    "            if tuple(new_head) in body_set and new_head != list(self.snake)[-1]:\n",
    "                reward -= 10\n",
    "                done = True\n",
    "\n",
    "        if not done:\n",
    "            self.snake.appendleft(new_head)\n",
    "\n",
    "            # --- Ate food ---\n",
    "            if new_head == self.food:\n",
    "                base = 10\n",
    "                time_bonus = max(15, 25 - 0.1 * self.steps_since_last_food)\n",
    "                reward += base + time_bonus\n",
    "                self.steps_since_last_food = 0\n",
    "                self._place_food()\n",
    "                info[\"food_eaten\"] = True\n",
    "            else:\n",
    "                self.snake.pop()\n",
    "                info[\"food_eaten\"] = False\n",
    "\n",
    "            # --- Moving closer or farther ---\n",
    "            dist_prev = abs(head[0] - self.food[0]) + abs(head[1] - self.food[1])\n",
    "            dist_new = abs(new_head[0] - self.food[0]) + abs(new_head[1] - self.food[1])\n",
    "            if dist_new < dist_prev:\n",
    "                reward += 0.3\n",
    "            elif dist_new > dist_prev:\n",
    "                reward -= 0.3\n",
    "        else:\n",
    "            info[\"food_eaten\"] = False\n",
    "            self.steps_since_last_food = 0  # reset counter\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        head_grid = np.zeros((self.size, self.size), dtype=np.float32)\n",
    "        body_grid = np.zeros((self.size, self.size), dtype=np.float32)\n",
    "        food_grid = np.zeros((self.size, self.size), dtype=np.float32)\n",
    "\n",
    "        for part in list(self.snake)[1:]:\n",
    "            body_grid[part[0], part[1]] = 1.0\n",
    "        head_x, head_y = self.snake[0]\n",
    "        head_grid[head_x, head_y] = 1.0\n",
    "        if self.food:\n",
    "            food_grid[self.food[0], self.food[1]] = 1.0\n",
    "\n",
    "        stacked = np.stack([head_grid, body_grid, food_grid], axis=0)\n",
    "        return stacked.astype(np.float32)\n",
    "\n",
    "    def _place_food(self):\n",
    "        positions = set(tuple(p) for p in self.snake)\n",
    "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
    "        if empty:\n",
    "            self.food = list(random.choice(empty))\n",
    "        else:\n",
    "            self.food = None  # grid full (shouldn't usually happen)\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            self._render_init()\n",
    "\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        for x, y in self.snake:\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (0, 255, 0),\n",
    "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "        if self.food:\n",
    "            fx, fy = self.food\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (255, 0, 0),\n",
    "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def _render_init(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen:\n",
    "            pygame.quit()\n",
    "            self.screen = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ffb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for Snake Game\n",
    "env = SnakeGame(render_mode=None)\n",
    "memory = deque(maxlen=10000)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "learning_rate = 1e-3\n",
    "target_update_freq = 10\n",
    "num_episodes = 500\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state_space = np.prod(env.observation_space.shape)\n",
    "policy_net = DQN(state_space, env.action_space.n).to(device)\n",
    "target_net = DQN(state_space, env.action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "replay_buffer = ExperienceBuffer(capacity=10000)\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995  # slower decay\n",
    "epsilon_min = 0.05     # exploration retained longer\n",
    "max_steps_per_episode = 500  # allow longer runs if improving\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).view(1, -1)\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    food_collected = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and steps < max_steps_per_episode:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "        # Encourage survival slightly and penalize dying\n",
    "        if done and reward == 0:\n",
    "            reward = -0.1\n",
    "        else:\n",
    "            reward += 0.01  # small bonus for being alive\n",
    "\n",
    "        # Count food collected based on info dict (assuming 'food_eaten' flag is set in env)\n",
    "        if info.get(\"food_eaten\", False):\n",
    "            food_collected += 1\n",
    "            reward += 1.0  # ensure food reward is significant\n",
    "\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0).view(1, -1)\n",
    "        replay_buffer.push(state, action, reward, next_state_tensor, done)\n",
    "        state = next_state_tensor\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            states = torch.cat(states).view(batch_size, -1)\n",
    "            next_states = torch.cat(next_states).view(batch_size, -1)\n",
    "            actions = torch.tensor(actions, device=device).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            curr_q_values = policy_net(states).gather(1, actions)\n",
    "            next_q_values = target_net(next_states).max(1)[0].detach().unsqueeze(1)\n",
    "            expected_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = F.mse_loss(curr_q_values, expected_q_values)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decay epsilon once per episode\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.3f}, Steps: {steps}, Food: {food_collected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e10565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_snake_model(model, size=20, episodes=10, render=True):\n",
    "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
    "    model.eval()\n",
    "\n",
    "    rewards = []\n",
    "    foods = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        food_collected = 0\n",
    "        done = False\n",
    "        max_steps = 500\n",
    "        steps = 0\n",
    "        while not done and steps < max_steps:\n",
    "            steps += 1\n",
    "            obs_resized = F.interpolate(torch.tensor(obs, dtype=torch.float32).unsqueeze(0), size=(10, 10), mode='bilinear')\n",
    "            state = obs_resized.view(1, -1)[:, :300]\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # ✅ Count food collected if info dict has that flag\n",
    "            if isinstance(info, dict) and info.get(\"food_eaten\", False):\n",
    "                food_collected += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        foods.append(food_collected)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}, Food Collected = {food_collected}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "    avg_food = sum(foods) / episodes\n",
    "\n",
    "    print(f\"\\nAverage reward over {episodes} episodes: {avg_reward:.2f}\")\n",
    "    print(f\"Average food collected: {avg_food:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7eb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run evaluation for Snake Game here\n",
    "evaluate_snake_model(policy_net, size=20, episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChaseEscapeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dt = 0.1\n",
    "        self.max_speed = 0.4\n",
    "        self.agent_radius = 0.05\n",
    "        self.target_radius = 0.05\n",
    "        self.chaser_radius = 0.05\n",
    "        self.chaser_speed = 0.03\n",
    "\n",
    "        self.action_space = gym.spaces.MultiDiscrete([3, 3])  # actions in {0,1,2} map to [-1,0,1]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(8,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_size = 500\n",
    "        self.np_random = None\n",
    "\n",
    "        if render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def sample_pos(self, far_from=None, min_dist=0.5):\n",
    "        while True:\n",
    "            pos = self.np_random.uniform(low=-0.8, high=0.8, size=(2,))\n",
    "            if far_from is None or np.linalg.norm(pos - far_from) >= min_dist:\n",
    "                return pos\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.agent_pos = self.sample_pos()\n",
    "        self.agent_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "        self.chaser_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.7)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # TODO: Decide how to pass the state (don't use pixel values)\n",
    "        agent_x, agent_y = self.agent_pos\n",
    "        agent_vx, agent_vy = self.agent_vel\n",
    "        target_x, target_y = self.target_pos\n",
    "        chaser_x, chaser_y = self.chaser_pos\n",
    "        return np.array([agent_x, agent_y, agent_vx, agent_vy, target_x, target_y, chaser_x, chaser_y], dtype=np.float32)\n",
    "\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: Add reward scheme\n",
    "        # 1) Try to make the agent stay within bounds\n",
    "        # 2) The agent shouldn't idle around\n",
    "        # 3) The agent should go for the reward\n",
    "        # 4) The agent should avoid the chaser\n",
    "        reward = 0.0\n",
    "        prev_dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
    "        accel = (np.array(action) - 1) * 0.1\n",
    "        self.agent_vel += accel\n",
    "        self.agent_vel = np.clip(self.agent_vel, -self.max_speed, self.max_speed)\n",
    "        # Save intended new position before clipping\n",
    "        new_pos = self.agent_pos.copy()\n",
    "        new_pos += self.agent_vel * self.dt\n",
    "\n",
    "        # Penalize if new position goes out of bounds\n",
    "        if np.any(new_pos < -1) or np.any(new_pos > 1):\n",
    "            reward -= 0.1\n",
    "\n",
    "        # Apply clipping to stay within bounds\n",
    "        self.agent_pos = np.clip(new_pos, -1, 1)\n",
    "\n",
    "        if np.linalg.norm(self.agent_vel) < 0.01:\n",
    "            reward -= 0.05  # or stronger penalty if needed\n",
    "\n",
    "        direction = self.agent_pos - self.chaser_pos\n",
    "        norm = np.linalg.norm(direction)\n",
    "        if norm > 1e-5:\n",
    "            self.chaser_pos += self.chaser_speed * direction / norm\n",
    "\n",
    "        dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
    "        dist_to_chaser = np.linalg.norm(self.agent_pos - self.chaser_pos)\n",
    "\n",
    "        if dist_to_chaser < 0.5:\n",
    "            if norm > 1e-5:\n",
    "                projected_chaser_pos = self.chaser_pos - self.chaser_speed * direction / norm\n",
    "                prev_dist_to_chaser = np.linalg.norm(self.agent_pos - projected_chaser_pos)\n",
    "            else:\n",
    "                prev_dist_to_chaser = dist_to_chaser\n",
    "\n",
    "            delta_chaser = prev_dist_to_chaser - dist_to_chaser\n",
    "            reward += delta_chaser * 0.3  # penalize moving toward chaser only when close\n",
    "\n",
    "        delta = prev_dist_to_target - dist_to_target\n",
    "        reward += delta * 0.5  # scale it to keep values small\n",
    "\n",
    "\n",
    "        terminated = False\n",
    "\n",
    "        info = {}\n",
    "        if dist_to_target < self.agent_radius + self.target_radius:\n",
    "            reward += 10.0\n",
    "            self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "            info[\"target_captured\"] = True\n",
    "\n",
    "        if dist_to_target < 0.3:\n",
    "            reward += 0.1\n",
    "\n",
    "        if dist_to_chaser < self.agent_radius + self.chaser_radius:\n",
    "            reward -= 3.0\n",
    "            terminated = True\n",
    "            info[\"caught_by_chaser\"] = True\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.close()\n",
    "\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        def to_screen(p):\n",
    "            x = int((p[0] + 1) / 2 * self.screen_size)\n",
    "            y = int((1 - (p[1] + 1) / 2) * self.screen_size)\n",
    "            return x, y\n",
    "\n",
    "        pygame.draw.circle(self.screen, (0, 255, 0), to_screen(self.target_pos), int(self.target_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (0, 0, 255), to_screen(self.agent_pos), int(self.agent_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), to_screen(self.chaser_pos), int(self.chaser_radius * self.screen_size))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and evaluate CatMouseEnv\n",
    "def select_action(state, model, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return [random.randint(0, 2), random.randint(0, 2)]\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        action = q_values.argmax(dim=1).item()\n",
    "        return [action // 3, action % 3]  # Convert flat index to (x,y) action\n",
    "\n",
    "env = ChaseEscapeEnv()\n",
    "dqn = DQN(state_space=8, action_space=9)  # 3x3 = 9 actions\n",
    "target_dqn = DQN(8, 9)\n",
    "target_dqn.load_state_dict(dqn.state_dict())\n",
    "optimizer = torch.optim.Adam(dqn.parameters(), lr=1e-3)\n",
    "buffer = ExperienceBuffer(capacity=10000)\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "epsilon_decay = 0.995\n",
    "max_steps = 500\n",
    "\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    target_captures = 0\n",
    "    caught_by_chaser = False\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        steps += 1\n",
    "        action = select_action(state, dqn, epsilon, env.action_space)\n",
    "        flat_action = action[0] * 3 + action[1]\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        buffer.push(state, flat_action, reward, next_state, terminated)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        done = terminated\n",
    "\n",
    "        # Track extra info\n",
    "        if info.get(\"target_captured\"):\n",
    "            target_captures += 1\n",
    "        if info.get(\"caught_by_chaser\"):\n",
    "            caught_by_chaser = True\n",
    "\n",
    "        # Train if enough samples\n",
    "        if len(buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            states = torch.FloatTensor(states)\n",
    "            actions = torch.LongTensor(actions)\n",
    "            rewards = torch.FloatTensor(rewards)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            dones = torch.FloatTensor(dones)\n",
    "\n",
    "            q_values = dqn(states)\n",
    "            next_q_values = target_dqn(next_states)\n",
    "\n",
    "            q_target = rewards + gamma * next_q_values.max(1)[0] * (1 - dones)\n",
    "            q_expected = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            loss = F.mse_loss(q_expected, q_target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        target_dqn.load_state_dict(dqn.state_dict())\n",
    "\n",
    "    # ✅ Clean, compact log per episode\n",
    "    print(\n",
    "        f\"Ep {episode:03d} | \"\n",
    "        f\"Reward: {total_reward:.2f} | \"\n",
    "        f\"Steps: {steps} | \"\n",
    "        f\"Captures: {target_captures} | \"\n",
    "        f\"Caught: {caught_by_chaser}\"\n",
    "    )\n",
    "\n",
    "# Evaluation phase\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = select_action(state, dqn, epsilon=0.0, action_space=env.action_space)\n",
    "    state, reward, terminated, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated\n",
    "    pygame.time.delay(50)\n",
    "\n",
    "print(f\"Total reward in evaluation: {total_reward:.2f}\")\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 BotB (venv)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
